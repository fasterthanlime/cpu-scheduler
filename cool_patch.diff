diff -uNp a/kernel/sched.c b/kernel/sched.c
--- a/kernel/sched.c	2010-02-24 19:52:17.000000000 +0100
+++ b/kernel/sched.c	2010-04-11 23:39:48.000000000 +0200
@@ -131,6 +131,19 @@ static inline int task_has_rt_policy(str
 	return rt_policy(p->policy);
 }
 
+static inline int dummy_policy(int policy)
+{
+	if (unlikely(policy == SCHED_DUMMY))
+		return 1;
+	return 0;
+}
+
+static inline int task_has_dummy_policy(struct task_struct *p)
+{
+	return dummy_policy(p->policy);
+}
+
+
 /*
  * This is the priority-queue data structure of the RT scheduling class:
  */
@@ -448,6 +461,12 @@ struct cfs_rq {
 #endif
 };
 
+/* Dummy classes' related field in a runqueue: */
+struct dummy_rq {
+	struct list_head list;
+	struct task_struct *task;
+};
+
 /* Real-Time classes' related field in a runqueue: */
 struct rt_rq {
 	struct rt_prio_array active;
@@ -544,6 +563,7 @@ struct rq {
 
 	struct cfs_rq cfs;
 	struct rt_rq rt;
+	struct dummy_rq dummy_rq;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this cpu: */
@@ -1837,6 +1857,7 @@ static inline void __set_task_cpu(struct
 #include "sched_stats.h"
 #include "sched_idletask.c"
 #include "sched_fair.c"
+#include "sched_dummy.c"
 #include "sched_rt.c"
 #ifdef CONFIG_SCHED_DEBUG
 # include "sched_debug.c"
@@ -1858,7 +1879,7 @@ static void dec_nr_running(struct rq *rq
 
 static void set_load_weight(struct task_struct *p)
 {
-	if (task_has_rt_policy(p)) {
+	if (task_has_rt_policy(p) || task_has_dummy_policy(p)) {
 		p->se.load.weight = prio_to_weight[0] * 2;
 		p->se.load.inv_weight = prio_to_wmult[0] >> 1;
 		return;
@@ -1952,6 +1973,9 @@ static int effective_prio(struct task_st
 	 * keep the priority unchanged. Otherwise, update priority
 	 * to the normal priority:
 	 */
+	if (unlikely(dummy_task(p)))
+		return p->prio;
+
 	if (!rt_prio(p->prio))
 		return p->normal_prio;
 	return p->prio;
@@ -2589,7 +2613,8 @@ void sched_fork(struct task_struct *p, i
 	 * Revert to default priority/policy on fork if requested.
 	 */
 	if (unlikely(p->sched_reset_on_fork)) {
-		if (p->policy == SCHED_FIFO || p->policy == SCHED_RR) {
+		if (p->policy == SCHED_FIFO || p->policy == SCHED_RR ||
+			dummy_policy(p->policy)) {
 			p->policy = SCHED_NORMAL;
 			p->normal_prio = p->static_prio;
 		}
@@ -2612,8 +2637,13 @@ void sched_fork(struct task_struct *p, i
 	 */
 	p->prio = current->normal_prio;
 
-	if (!rt_prio(p->prio))
-		p->sched_class = &fair_sched_class;
+	if (!rt_prio(p->prio)) {
+		if (dummy_prio(p->prio)) {
+			p->sched_class = &dummy_sched_class;
+		} else {
+			p->sched_class = &fair_sched_class;
+		}
+	}
 
 	if (p->sched_class->task_fork)
 		p->sched_class->task_fork(p);
@@ -6069,10 +6099,15 @@ void rt_mutex_setprio(struct task_struct
 	if (running)
 		p->sched_class->put_prev_task(rq, p);
 
-	if (rt_prio(prio))
+	if (rt_prio(prio)) {
 		p->sched_class = &rt_sched_class;
-	else
-		p->sched_class = &fair_sched_class;
+	} else {
+		if (dummy_prio(prio)) {
+			p->sched_class = &dummy_sched_class;
+		} else {
+			p->sched_class = &fair_sched_class;
+		}
+	}
 
 	p->prio = prio;
 
@@ -6108,7 +6143,7 @@ void set_user_nice(struct task_struct *p
 	 * it wont have any effect on scheduling until the task is
 	 * SCHED_FIFO/SCHED_RR:
 	 */
-	if (task_has_rt_policy(p)) {
+	if (unlikely(task_has_rt_policy(p) || task_has_dummy_policy(p))) {
 		p->static_prio = NICE_TO_PRIO(nice);
 		goto out_unlock;
 	}
@@ -6120,6 +6155,11 @@ void set_user_nice(struct task_struct *p
 	set_load_weight(p);
 	old_prio = p->prio;
 	p->prio = effective_prio(p);
+
+	if (dummy_prio(p->prio)) {
+		p->sched_class = &dummy_sched_class;
+	}
+
 	delta = p->prio - old_prio;
 
 	if (on_rq) {
@@ -6253,10 +6293,13 @@ __setscheduler(struct rq *rq, struct tas
 	p->normal_prio = normal_prio(p);
 	/* we are holding p->pi_lock already */
 	p->prio = rt_mutex_getprio(p);
-	if (rt_prio(p->prio))
+	if (rt_prio(p->prio)) {
 		p->sched_class = &rt_sched_class;
-	else
+	} else if (dummy_prio(p->prio)) {
+		p->sched_class = &dummy_sched_class;
+	} else {
 		p->sched_class = &fair_sched_class;
+	}
 	set_load_weight(p);
 }
 
@@ -6298,7 +6341,7 @@ recheck:
 
 		if (policy != SCHED_FIFO && policy != SCHED_RR &&
 				policy != SCHED_NORMAL && policy != SCHED_BATCH &&
-				policy != SCHED_IDLE)
+				policy != SCHED_IDLE && policy != SCHED_DUMMY)
 			return -EINVAL;
 	}
 
@@ -6311,6 +6354,10 @@ recheck:
 	    (p->mm && param->sched_priority > MAX_USER_RT_PRIO-1) ||
 	    (!p->mm && param->sched_priority > MAX_RT_PRIO-1))
 		return -EINVAL;
+
+	if (dummy_policy(policy) && (param->sched_priority != 0))
+		return -EINVAL;
+		
 	if (rt_policy(policy) != (param->sched_priority != 0))
 		return -EINVAL;
 
@@ -9377,6 +9424,13 @@ static void init_cfs_rq(struct cfs_rq *c
 	cfs_rq->min_vruntime = (u64)(-(1LL << 20));
 }
 
+
+static void init_dummy_rq(struct dummy_rq *dummy_rq, struct rq *rq)
+{
+	INIT_LIST_HEAD(&dummy_rq->list);
+}
+
+
 static void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq)
 {
 	struct rt_prio_array *array;
@@ -9570,6 +9624,7 @@ void __init sched_init(void)
 		rq->calc_load_active = 0;
 		rq->calc_load_update = jiffies + LOAD_FREQ;
 		init_cfs_rq(&rq->cfs, rq);
+		init_dummy_rq(&rq->dummy_rq, rq);
 		init_rt_rq(&rq->rt, rq);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 		init_task_group.shares = init_task_group_load;
@@ -9685,7 +9740,11 @@ void __init sched_init(void)
 	/*
 	 * During early bootup we pretend to be a normal task:
 	 */
-	current->sched_class = &fair_sched_class;
+	if (unlikely(current->prio > MAX_PRIO-10)) {
+		current->sched_class = &dummy_sched_class;
+	} else {
+		current->sched_class = &fair_sched_class;
+	}
 
 	/* Allocate the nohz_cpu_mask if CONFIG_CPUMASK_OFFSTACK */
 	zalloc_cpumask_var(&nohz_cpu_mask, GFP_NOWAIT);
diff -uNp a/kernel/sched_dummy.c b/kernel/sched_dummy.c
--- a/kernel/sched_dummy.c	1970-01-01 01:00:00.000000000 +0100
+++ b/kernel/sched_dummy.c	2010-04-12 17:59:51.000000000 +0200
@@ -0,0 +1,121 @@
+/**
+ * Dummy scheduler
+ */
+#include <linux/sched.h>
+
+
+static void _enqueue(struct rq *rq, struct task_struct *p)
+{
+	struct dummy_rq *n = kmalloc(sizeof(struct dummy_rq), GFP_KERNEL);
+	n->task = p;
+
+	list_add_tail(&(n->list), &(rq->dummy_rq.list));
+}
+
+static void _remove(struct rq *rq, struct task_struct *p)
+{
+	struct list_head *pos, *q;
+	struct dummy_rq *tmp;
+
+	list_for_each_safe(pos, q, &rq->dummy_rq.list) {
+		tmp = list_entry(pos, struct dummy_rq, list);
+		if (tmp->task == p) {
+			list_del(pos);
+			kfree(tmp);
+		}
+	}
+}
+
+/**
+ * Schedule classes to implement:
+ * - enqueue_task_dummy
+ *   task starts being handled by the dummy scheduler
+ *
+ * - dequeue_task_dummy
+ *   task stops being handled by the dummy scheduler
+ *
+ * - pick_next_task_dummy
+ *   select the next task to switch to
+ *
+ * - put_prev_task_dummy
+ *   handles tasks that have been preempted
+ *
+ * - task_tick_dummy
+ *   should a task be preempted?
+ *
+ * - prio_changed_dummy
+ *   some task changed its priority
+ */
+
+static void enqueue_task_dummy(struct rq *rq, struct task_struct *p, int wakeup)
+{
+	_enqueue(rq, p);
+}
+static void dequeue_task_dummy(struct rq *rq, struct task_struct *p, int sleep)
+{
+	_remove(rq, p);
+}
+static struct task_struct *pick_next_task_dummy(struct rq *rq)
+{
+	if (!list_empty(&rq->dummy_rq.list)) {
+		return list_entry(rq->dummy_rq.list.next, struct dummy_rq, list)->task;
+	}
+	return NULL;
+}
+
+static void put_prev_task_dummy(struct rq *rq, struct task_struct *prev)
+{
+	// to implement
+}
+
+static void task_tick_dummy(struct rq *rq, struct task_struct *curr, int queued)
+{
+	// to implement
+}
+
+static void prio_changed_dummy(struct rq *rq, struct task_struct *p,
+    int oldprio, int running)
+{
+	// to implement
+}
+
+/**
+ * You can ignore the following functions...
+ */
+
+static void yield_task_dummy(struct rq *rq) { }
+
+static void check_preempt_wakeup_dummy(struct rq *rq, struct task_struct *p, int wake_flags) { }
+
+static void set_curr_task_dummy(struct rq *rq) { }
+
+static void task_fork_dummy(struct task_struct *p) { }
+
+static void switched_to_dummy(struct rq *rq, struct task_struct *p,
+    int running)
+{
+	if (!running) {
+		resched_task(p);
+	}
+}
+
+static const struct sched_class dummy_sched_class = {
+	.next		    = &fair_sched_class,
+	.enqueue_task       = enqueue_task_dummy,
+	.dequeue_task       = dequeue_task_dummy,
+	.yield_task         = yield_task_dummy,
+
+	.check_preempt_curr = check_preempt_wakeup_dummy,
+
+	.pick_next_task     = pick_next_task_dummy,
+	.put_prev_task      = put_prev_task_dummy,
+
+	.set_curr_task      = set_curr_task_dummy,
+	.task_tick          = task_tick_dummy,
+	.task_fork          = task_fork_dummy,
+
+	.prio_changed       = prio_changed_dummy,
+	.switched_to        = switched_to_dummy,
+
+	.get_rr_interval    = get_rr_interval_fair,
+};
diff -uNp a/kernel/sched_rt.c b/kernel/sched_rt.c
--- a/kernel/sched_rt.c	2010-02-24 19:52:17.000000000 +0100
+++ b/kernel/sched_rt.c	2010-04-11 15:56:56.000000000 +0200
@@ -1733,7 +1733,7 @@ unsigned int get_rr_interval_rt(struct r
 }
 
 static const struct sched_class rt_sched_class = {
-	.next			= &fair_sched_class,
+	.next			= &dummy_sched_class,
 	.enqueue_task		= enqueue_task_rt,
 	.dequeue_task		= dequeue_task_rt,
 	.yield_task		= yield_task_rt,
diff -uNp a/include/linux/sched.h b/include/linux/sched.h
--- a/include/linux/sched.h	2010-02-24 19:52:17.000000000 +0100
+++ b/include/linux/sched.h	2010-04-12 17:34:42.000000000 +0200
@@ -38,6 +38,7 @@
 #define SCHED_BATCH		3
 /* SCHED_ISO: reserved but not implemented yet */
 #define SCHED_IDLE		5
+#define SCHED_DUMMY		6
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
 
@@ -1602,6 +1603,16 @@ static inline int rt_task(struct task_st
 	return rt_prio(p->prio);
 }
 
+static inline int dummy_prio(int prio)
+{
+	return !rt_prio(prio);
+}
+
+static inline int dummy_task(struct task_struct *p)
+{
+	return dummy_prio(p->prio);
+}
+
 static inline struct pid *task_pid(struct task_struct *task)
 {
 	return task->pids[PIDTYPE_PID].pid;
